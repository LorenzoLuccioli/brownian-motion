\chapter{Projective family of the Brownian motion}
\label{chap:projective_family}


\section{Kolmogorov extension theorem}

This theorem has been formalized by RÃ©my Degenne and Peter Pfaffelhuber in the repository \href{https://github.com/RemyDegenne/kolmogorov_extension4}{kolmogorov\_extension4}.

\begin{definition}[Projective family]\label{def:IsProjectiveMeasureFamily}
  \mathlibok
  \lean{MeasureTheory.IsProjectiveMeasureFamily}
A family of measures $P$ indexed by finite sets of $T$ is projective if, for finite sets $J \subseteq I$, the projection from $E^I$ to $E^J$ maps $P_I$ to $P_J$.
\end{definition}


\begin{definition}[Projective limit]\label{def:IsProjectiveLimit}
  \uses{def:IsProjectiveMeasureFamily}
  \mathlibok
  \lean{MeasureTheory.IsProjectiveLimit}
A measure $\mu$ on $E^T$ is the projective limit of a projective family of measures $P$ indexed by finite sets of $T$ if, for every finite set $I \subseteq T$, the projection from $E^T$ to $E^I$ maps $\mu$ to $P_I$.
\end{definition}


\begin{theorem}[Kolmogorov extension theorem]\label{thm:kolmogorovExtension}
  \uses{def:IsProjectiveLimit, def:IsProjectiveMeasureFamily}
  \leanok
  \lean{MeasureTheory.projectiveLimit, MeasureTheory.IsProjectiveLimit.unique, MeasureTheory.isProjectiveLimit_projectiveLimit, MeasureTheory.isFiniteMeasure_projectiveLimit, MeasureTheory.isProbabilityMeasure_projectiveLimit}
Let $\mathcal{X}$ be a Polish space, equipped with the Borel $\sigma$-algebra, and let $T$ be an index set.
Let $P$ be a projective family of finite measures on $\mathcal{X}$.
Then the projective limit $\mu$ of $P$ exists, is unique, and is a finite measure on $\mathcal{X}^T$.
Moreover, if $P_I$ is a probability measure for every finite set $I \subseteq T$, then $\mu$ is a probability measure.
\end{theorem}

\begin{proof}\leanok

\end{proof}


\section{Projective family of Gaussian measures}

We build a projective family of Gaussian measures indexed by $\mathbb{R}_+$.
In order to do so, we need to define specific Gaussian measures on finite index sets $\{t_1, \ldots, t_n\}$.
We want to build a multivariate Gaussian measure on $\mathbb{R}^n$ with mean $0$ and covariance matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.

% \paragraph{First method: Gaussian increments}

% In this method, we build the Gaussian measure by adding independent Gaussian increments.

% \begin{definition}(Gaussian increment)\label{def:gaussianIncrement}
% For $v \ge 0$, the map from $\mathbb{R}$ to the probability measures on $\mathbb{R}$ defined by $x \mapsto \mathcal{N}(x, v)$ is a Markov kernel.
% We call that kernel the \emph{Gaussian increment} with variance $v$ and denote it by $\kappa^G_v$.
% \end{definition}

% TODO: perhaps the equality $\mathcal{N}(x, v) = \delta_x \ast \mathcal{N}(0, v)$ is useful to show that it is a kernel?

% \begin{definition}\label{def:gaussianFromIncrements}
%   \uses{def:gaussianIncrement}
% Let $0 \le t_1 \le \ldots \le t_n$ be non-negative reals.
% Let $\mu_0$ be the real Gaussian distribution $\mathcal{N}(0, t_1)$.
% For $i \in \{1, \ldots, n-1\}$, let $\kappa_i$ be the Markov kernel from $\mathbb{R}$ to $\mathbb{R}$ defined by $\kappa_i(x) = \mathcal{N}(x, t_{i+1} - t_i)$ (the Gaussian increment $\kappa^G_{t_{i+1} - t_i}$).
% Let $P_{t_1, \ldots, t_n}$ be the measure on $\mathbb{R}^n$ defined by $\mu_0 \otimes \kappa_1 \otimes \ldots \otimes \kappa_{n-1}$.
% \end{definition}

% TODO: explain the notation $\otimes$ in the lemma above: $\kappa_{n-1}$ takes the value at $n-1$ only to produce the distribution at $n$.

% \begin{lemma}\label{lem:isGaussian_gaussianFromIncrements}
%   \uses{def:gaussianFromIncrements, def:IsGaussian}
% $P_{t_1, \ldots, t_n}$ is a Gaussian measure on $\mathbb{R}^n$ with mean $0$ and covariance matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
% \end{lemma}

% \begin{proof}

% \end{proof}


% \paragraph{Second method: covariance matrix}

We prove that the matrix $C_{ij} = \min(t_i, t_j)$ is positive semidefinite, which means that there exists a Gaussian distribution with mean 0 and covariance matrix $C$.

\begin{definition}[Gram matrix]\label{def:gramMatrix}
  \leanok
  \lean{Matrix.gram}
Let $v_1, \ldots, v_n$ be vectors in an inner product space $E$.
The Gram matrix of $v_1, \ldots, v_n$ is the matrix in $\mathbb{R}^{n \times n}$ with entries $G_{ij} = \langle v_i, v_j \rangle$ for $1 \leq i,j \leq n$.
\end{definition}


\begin{lemma}\label{lem:posSemidef_gramMatrix}
  \uses{def:gramMatrix}
  \leanok
  \lean{Matrix.gram_posSemidef}
A gram matrix is positive semidefinite.
\end{lemma}

\begin{proof}\leanok
Symmetry is obvious from the definition.
Let $x \in E$. Then
\begin{align*}
  \langle x, G x \rangle
  &= \sum_{i,j} x_i x_j \langle v_i, v_j \rangle
  \\
  &= \langle \sum_i x_i v_i, \sum_j x_j v_j \rangle
  \\
  &= \left\Vert \sum_i x_i v_i \right\Vert^2
  \\
  &\ge 0
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:C_eq_gramMatrix}
  \uses{def:gramMatrix}
  \leanok
Let $I = \{t_1, \ldots, t_n\}$ be a finite subset of $\mathbb{R}_+$.
For $i \le n$, let $v_i = \mathbb{I}_{[0, t_i]}$ be the indicator function of the interval $[0, t_i]$, as an element of $L^2(\mathbb{R})$.
Then the Gram matrix of $v_1, \ldots, v_n$ is equal to the matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
\end{lemma}

\begin{proof}\leanok
By definition of the inner product in $L^2(\mathbb{R})$,
\begin{align*}
  \langle v_i, v_j \rangle
  &= \int_{\mathbb{R}} \mathbb{I}_{[0, t_i]}(x) \mathbb{I}_{[0, t_j]}(x) \: dx
  = \min(t_i, t_j)
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:posSemidef_brownianCov}
  \leanok
  \lean{ProbabilityTheory.posSemidef_brownianCovMatrix}
For $I = \{t_1, \ldots, t_n\}$ a finite subset of $\mathbb{R}_+$, let $C \in \mathbb{R}^{n \times n}$ be the matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
Then $C$ is positive semidefinite.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:C_eq_gramMatrix, lem:posSemidef_gramMatrix}
$C$ is a Gram matrix by Lemma~\ref{lem:C_eq_gramMatrix}.
By Lemma~\ref{lem:posSemidef_gramMatrix}, it is positive semidefinite.
\end{proof}


\paragraph{Definition of the projective family and extension}

\begin{definition}[Projective family of the Brownian motion]\label{def:gaussianProjectiveFamily}
  \uses{def:multivariateGaussian, lem:posSemidef_brownianCov}
  \leanok
  \lean{ProbabilityTheory.gaussianProjectiveFamily}
For $I = \{t_1, \ldots, t_n\}$ a finite subset of $\mathbb{R}_+$, let $P^B_I$ be the multivariate Gaussian measure on $\mathbb{R}^n$ with mean $0$ and covariance matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
We call the family of measures $P^B_I$ the \emph{projective family of the Brownian motion}.
\end{definition}


\begin{lemma}\label{lem:isProjectiveMeasureFamily_gaussianProjectiveFamily}
  \uses{def:gaussianProjectiveFamily, def:IsProjectiveMeasureFamily}
  \leanok
  \lean{ProbabilityTheory.isProjectiveMeasureFamily_gaussianProjectiveFamily}
The projective family of the Brownian motion is a projective family of measures.
\end{lemma}

\begin{proof}\leanok
  \uses{lem:isGaussian_multivariateGaussian, lem:covMatrix_map,
  lem:integral_id_multivariateGaussian, lem:covMatrix_multivariateGaussian, lem:IsGaussian.ext_iff}
Let $J \subseteq I$ be finite subsets of $\mathbb{R}_+$.
We need to show that the restriction from $\mathbb{R}^I$ to $\mathbb{R}^J$ (denote it by $\pi_{IJ}$) maps $P^B_I$ to $P^B_J$.

The restriction is a continuous linear map from $\mathbb{R}^I$ to $\mathbb{R}^J$.
The map of a Gaussian measure by a continuous linear map is Gaussian (Lemma~\ref{lem:isGaussian_map}).
It thus suffices to show that the mean and covariance matrix of the map are equal to the ones of $P^B_J$ by Lemma~\ref{lem:IsGaussian.ext_iff}.

The mean of the map is $0$, since the mean of $P^B_I$ is $0$ and the map is linear.

Let us turn to the covariance matrix. For any $i \in J$, the map $x : \mathbb{R}^I \mapsto \pi_{IJ}(x) i$ is equal to $x : \mathbb{R}^I \mapsto x i$. Let $i, j \in J$. The covariance of $x : \mathbb{R}^J \mapsto x i$ and $x : \mathbb{R}^J \mapsto x j$ with respect to ${\pi_{IJ}}_*P^B_J$ is equal to the covariance of $x : \mathbb{R}^I \mapsto \pi_{IJ}(x) i$ and $x : \mathbb{R}^I \mapsto \pi_{IJ}(x) j$ with respect to $P^B_I$, which is equal to the covariance of $x : \mathbb{R}^I \mapsto x i$ and $x : \mathbb{R}^I \mapsto x i$ with respect to $P^B_I$, which is equal to $t_i \land t_j$. But this is also the covariance of $x : \mathbb{R}^J \mapsto x i$ and $x : \mathbb{R}^J \mapsto x j$ with respect to $P^B_J$, so we are done.
\end{proof}


\begin{definition}\label{def:gaussianLimit}
  \uses{thm:kolmogorovExtension, lem:isProjectiveMeasureFamily_gaussianProjectiveFamily}
  \leanok
  \lean{ProbabilityTheory.gaussianLimit}
We denote by $P_B$ the projective limit of the projective family of the Brownian motion given by Theorem~\ref{thm:kolmogorovExtension}.
This is a probability measure on $\mathbb{R}^{\mathbb{R}_+}$.
\end{definition}
